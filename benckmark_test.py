import requests
import time
import statistics
import os

# è¨­å®š API URL å’Œ Key
# å¦‚æžœä½ çš„ Docker æ˜¯è·‘åœ¨ 8000 port
API_URL = "http://localhost:8000/predict"
API_KEY = "frontend-dev-key"  # ä½¿ç”¨æˆ‘å€‘è¨­å®šçš„é è¨­ Key

# æ¸¬è©¦ç”¨çš„å‡è³‡æ–™
TEST_PAYLOAD = {"text": "This is a test comment to benchmark the API latency."}
HEADERS = {"X-API-Key": API_KEY}


def run_benchmark(num_requests=100):
    print(f"ðŸš€ é–‹å§‹åŸ·è¡Œ Benchmark (å£“åŠ›æ¸¬è©¦)... ç›®æ¨™: {num_requests} æ¬¡è«‹æ±‚")
    print(f"ðŸŽ¯ Target URL: {API_URL}")

    latencies = []
    errors = 0

    # 1. æš–æ©Ÿ (Warm-up)
    # ç¬¬ä¸€å€‹è«‹æ±‚é€šå¸¸æ¯”è¼ƒæ…¢ï¼ˆå› ç‚ºè¦å»ºç«‹é€£ç·šæˆ–è¼‰å…¥ Lazy Load çš„è³‡æºï¼‰ï¼Œæˆ‘å€‘ä¸è¨ˆå…¥çµ±è¨ˆ
    try:
        requests.post(API_URL, json=TEST_PAYLOAD, headers=HEADERS)
        print("ðŸ”¥ æš–æ©Ÿå®Œæˆ (Warm-up request sent)")
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£ç·šåˆ° API: {e}")
        print("è«‹ç¢ºèª Docker æ˜¯å¦æ­£åœ¨åŸ·è¡Œ (docker-compose up)")
        return

    # 2. é–‹å§‹æ­£å¼æ¸¬è©¦
    start_total_time = time.time()

    for i in range(num_requests):
        try:
            req_start = time.time()
            response = requests.post(API_URL, json=TEST_PAYLOAD, headers=HEADERS)
            req_end = time.time()

            # è¨ˆç®—è€—æ™‚ (æ¯«ç§’)
            latency_ms = (req_end - req_start) * 1000

            if response.status_code == 200:
                latencies.append(latency_ms)
                # print(f"è«‹æ±‚ {i+1}: {latency_ms:.2f} ms") # å¦‚æžœä¸æƒ³çœ‹å¤ªå¤šåˆ·å±å¯ä»¥è¨»è§£æŽ‰
            else:
                print(f"è«‹æ±‚ {i + 1} å¤±æ•—: Status {response.status_code}")
                errors += 1

        except Exception as e:
            print(f"è«‹æ±‚éŒ¯èª¤: {e}")
            errors += 1

    total_time = time.time() - start_total_time

    # 3. çµ±è¨ˆçµæžœ
    if latencies:
        avg_latency = statistics.mean(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th Percentile
        p99_latency = statistics.quantiles(latencies, n=100)[98]  # 99th Percentile
        min_latency = min(latencies)
        max_latency = max(latencies)

        print("\n" + "=" * 40)
        print("ðŸ“Š Benchmark æ¸¬è©¦çµæžœå ±å‘Š")
        print("=" * 40)
        print(f"âœ… æˆåŠŸè«‹æ±‚æ•¸: {len(latencies)} / {num_requests}")
        print(f"âŒ å¤±æ•—è«‹æ±‚æ•¸: {errors}")
        print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’")
        print("-" * 20)
        print(f"âš¡ å¹³å‡å»¶é² (Average Latency): {avg_latency:.2f} ms")
        print(f"âš¡ P95 å»¶é² (95% çš„è«‹æ±‚å¿«æ–¼):  {p95_latency:.2f} ms")
        print(f"âš¡ æœ€å¿«å›žæ‡‰: {min_latency:.2f} ms")
        print(f"âš¡ æœ€æ…¢å›žæ‡‰: {max_latency:.2f} ms")
        print("=" * 40)

        print("\nðŸ“ ã€ä½ å¯ä»¥ç›´æŽ¥è²¼åˆ°å±¥æ­·ä¸Šçš„å¥å­ã€‘ï¼š")
        print(
            f'> "Engineered a high-performance REST API handling inference requests with an average latency of {avg_latency:.0f}ms (P95 < {p95_latency:.0f}ms), utilizing FastAPI asynchronous workers."')
    else:
        print("æ²’æœ‰æ”¶é›†åˆ°æˆåŠŸçš„æ•¸æ“šã€‚")


if __name__ == "__main__":
    run_benchmark()